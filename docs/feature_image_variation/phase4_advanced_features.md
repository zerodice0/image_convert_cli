# Phase 4: ê³ ê¸‰ ê¸°ëŠ¥ ë° ìµœì í™”

## ğŸ“‹ ê°œìš”
ì´ë¯¸ì§€ ë³€í˜• ìƒì„± ê¸°ëŠ¥ì˜ í’ˆì§ˆ, ì„±ëŠ¥, ì•ˆì •ì„±ì„ í–¥ìƒì‹œí‚¤ëŠ” ê³ ê¸‰ ê¸°ëŠ¥ë“¤ì„ êµ¬í˜„í•˜ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤.

## ğŸ¯ ëª©í‘œ
- ë³€í˜• í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ êµ¬í˜„
- ì¤‘ë³µ ê²°ê³¼ ë°©ì§€ ì•Œê³ ë¦¬ì¦˜ ê°œë°œ
- ì„±ëŠ¥ ìµœì í™” ë° ë©”ëª¨ë¦¬ ê´€ë¦¬
- ê³ ê¸‰ ì˜¤ë¥˜ ì²˜ë¦¬ ë° ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜

## ğŸ” ë³€í˜• í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ

### 1. í’ˆì§ˆ ë©”íŠ¸ë¦­ ì •ì˜
```python
class VariationQualityAnalyzer:
    """ë³€í˜• í’ˆì§ˆ ë¶„ì„ ë° ê²€ì¦"""
    
    def __init__(self):
        self.quality_thresholds = {
            'similarity_min': 0.3,  # ì›ë³¸ê³¼ì˜ ìµœì†Œ ìœ ì‚¬ë„
            'similarity_max': 0.9,  # ì›ë³¸ê³¼ì˜ ìµœëŒ€ ìœ ì‚¬ë„
            'diversity_min': 0.2,   # ë‹¤ë¥¸ ë³€í˜•ë“¤ê³¼ì˜ ìµœì†Œ ì°¨ì´
            'aesthetic_min': 0.6,   # ë¯¸ì  í’ˆì§ˆ ìµœì†Œ ê¸°ì¤€
            'object_integrity': 0.7  # ê°ì²´ ì™„ì „ì„± ê¸°ì¤€
        }
    
    def analyze_variation_quality(self, original_image: Image, 
                                variation_image: Image, 
                                other_variations: List[Image] = None) -> Dict[str, float]:
        """ë³€í˜• í’ˆì§ˆ ì¢…í•© ë¶„ì„"""
        metrics = {}
        
        # 1. ì›ë³¸ê³¼ì˜ ìœ ì‚¬ë„ ë¶„ì„
        metrics['similarity'] = self.calculate_similarity(original_image, variation_image)
        
        # 2. ë‹¤ì–‘ì„± ë¶„ì„ (ë‹¤ë¥¸ ë³€í˜•ë“¤ê³¼ì˜ ì°¨ì´)
        if other_variations:
            metrics['diversity'] = self.calculate_diversity(variation_image, other_variations)
        
        # 3. ë¯¸ì  í’ˆì§ˆ ë¶„ì„
        metrics['aesthetic'] = self.analyze_aesthetic_quality(variation_image)
        
        # 4. ê°ì²´ ì™„ì „ì„± ê²€ì¦
        metrics['object_integrity'] = self.verify_object_integrity(variation_image)
        
        # 5. ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        metrics['overall_quality'] = self.calculate_overall_quality(metrics)
        
        return metrics
    
    def calculate_similarity(self, img1: Image, img2: Image) -> float:
        """ì´ë¯¸ì§€ ê°„ ìœ ì‚¬ë„ ê³„ì‚° (SSIM ê¸°ë°˜)"""
        from skimage.metrics import structural_similarity as ssim
        import numpy as np
        
        # ì´ë¯¸ì§€ë¥¼ ê·¸ë ˆì´ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
        img1_gray = np.array(img1.convert('L'))
        img2_gray = np.array(img2.convert('L'))
        
        # í¬ê¸° í†µì¼
        if img1_gray.shape != img2_gray.shape:
            img2_resized = cv2.resize(img2_gray, img1_gray.shape[::-1])
        else:
            img2_resized = img2_gray
        
        # SSIM ê³„ì‚°
        similarity_score = ssim(img1_gray, img2_resized)
        return similarity_score
    
    def calculate_diversity(self, target_img: Image, other_imgs: List[Image]) -> float:
        """ë³€í˜•ë“¤ ê°„ ë‹¤ì–‘ì„± ê³„ì‚°"""
        if not other_imgs:
            return 1.0
        
        # ê° ì´ë¯¸ì§€ì™€ì˜ ì°¨ì´ ê³„ì‚°
        differences = []
        for other_img in other_imgs:
            similarity = self.calculate_similarity(target_img, other_img)
            differences.append(1.0 - similarity)  # ì°¨ì´ = 1 - ìœ ì‚¬ë„
        
        # í‰ê·  ì°¨ì´ë„ ë°˜í™˜
        return sum(differences) / len(differences)
    
    def analyze_aesthetic_quality(self, image: Image) -> float:
        """ë¯¸ì  í’ˆì§ˆ ë¶„ì„"""
        import numpy as np
        from PIL import ImageStat
        
        # ê¸°ë³¸ ì´ë¯¸ì§€ í†µê³„
        stat = ImageStat.Stat(image)
        
        metrics = {}
        
        # 1. ìƒ‰ìƒ ë¶„ì‚° (ë‹¤ì±„ë¡œìš´ ìƒ‰ìƒ ì„ í˜¸)
        color_variance = np.var([stat.var[i] for i in range(min(3, len(stat.var)))])
        metrics['color_variance'] = min(color_variance / 10000, 1.0)
        
        # 2. ëŒ€ë¹„ ë¶„ì„
        img_array = np.array(image.convert('L'))
        contrast = np.std(img_array) / 255.0
        metrics['contrast'] = min(contrast * 2, 1.0)
        
        # 3. ë°ê¸° ê· í˜•
        brightness = np.mean(img_array) / 255.0
        brightness_balance = 1.0 - abs(brightness - 0.5) * 2
        metrics['brightness_balance'] = brightness_balance
        
        # 4. ì„ ëª…ë„ (Laplacian variance)
        laplacian_var = cv2.Laplacian(img_array, cv2.CV_64F).var()
        sharpness = min(laplacian_var / 1000, 1.0)
        metrics['sharpness'] = sharpness
        
        # ì¢…í•© ë¯¸ì  ì ìˆ˜
        weights = {
            'color_variance': 0.25,
            'contrast': 0.25,
            'brightness_balance': 0.25,
            'sharpness': 0.25
        }
        
        aesthetic_score = sum(metrics[key] * weights[key] for key in weights)
        return aesthetic_score
    
    def verify_object_integrity(self, image: Image) -> float:
        """ê°ì²´ ì™„ì „ì„± ê²€ì¦"""
        # ê°„ë‹¨í•œ ì—£ì§€ ê²€ì¶œ ê¸°ë°˜ ê°ì²´ ì™„ì „ì„± í™•ì¸
        import cv2
        import numpy as np
        
        img_array = np.array(image.convert('L'))
        
        # Canny ì—£ì§€ ê²€ì¶œ
        edges = cv2.Canny(img_array, 50, 150)
        
        # ì—°ê²°ëœ ì»´í¬ë„ŒíŠ¸ ë¶„ì„
        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(edges)
        
        # ê°ì²´ ì™„ì „ì„± ì ìˆ˜ (ì—°ê²°ëœ ì˜ì—­ì˜ í¬ê¸°ì™€ ê°œìˆ˜ ê¸°ë°˜)
        if num_labels > 1:  # ë°°ê²½ ì œì™¸
            # í° ì—°ê²° ì˜ì—­ë“¤ì˜ ë¹„ìœ¨
            areas = stats[1:, cv2.CC_STAT_AREA]  # ë°°ê²½(ë¼ë²¨ 0) ì œì™¸
            total_area = image.size[0] * image.size[1]
            large_areas = areas[areas > total_area * 0.01]  # ì „ì²´ì˜ 1% ì´ìƒ
            integrity = min(len(large_areas) / 10, 1.0)
        else:
            integrity = 0.1  # ê±°ì˜ ì—£ì§€ê°€ ì—†ëŠ” ê²½ìš°
        
        return integrity
    
    def calculate_overall_quality(self, metrics: Dict[str, float]) -> float:
        """ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        weights = {
            'similarity': 0.2,      # ì›ë³¸ê³¼ì˜ ì ì ˆí•œ ìœ ì‚¬ì„±
            'diversity': 0.3,       # ë‹¤ë¥¸ ë³€í˜•ë“¤ê³¼ì˜ ì°¨ë³„ì„±
            'aesthetic': 0.3,       # ë¯¸ì  í’ˆì§ˆ
            'object_integrity': 0.2  # ê°ì²´ ì™„ì „ì„±
        }
        
        # ìœ ì‚¬ë„ëŠ” ì ì • ë²”ìœ„ì— ìˆì„ ë•Œ ë†’ì€ ì ìˆ˜
        similarity_score = metrics.get('similarity', 0.5)
        if 0.3 <= similarity_score <= 0.9:
            similarity_weighted = 1.0 - abs(similarity_score - 0.6) / 0.3
        else:
            similarity_weighted = 0.1
        
        weighted_metrics = {
            'similarity': similarity_weighted,
            'diversity': metrics.get('diversity', 0.5),
            'aesthetic': metrics.get('aesthetic', 0.5),
            'object_integrity': metrics.get('object_integrity', 0.5)
        }
        
        overall = sum(weighted_metrics[key] * weights[key] for key in weights)
        return overall

    def is_acceptable_quality(self, metrics: Dict[str, float]) -> bool:
        """í’ˆì§ˆ ê¸°ì¤€ í†µê³¼ ì—¬ë¶€ íŒë‹¨"""
        checks = [
            self.quality_thresholds['similarity_min'] <= metrics.get('similarity', 0) <= self.quality_thresholds['similarity_max'],
            metrics.get('diversity', 0) >= self.quality_thresholds['diversity_min'],
            metrics.get('aesthetic', 0) >= self.quality_thresholds['aesthetic_min'],
            metrics.get('object_integrity', 0) >= self.quality_thresholds['object_integrity'],
            metrics.get('overall_quality', 0) >= 0.6
        ]
        
        return all(checks)
```

### 2. ì¤‘ë³µ ê²°ê³¼ ë°©ì§€ ì‹œìŠ¤í…œ
```python
class DuplicationPreventer:
    """ë³€í˜• ì¤‘ë³µ ë°©ì§€ ì‹œìŠ¤í…œ"""
    
    def __init__(self, similarity_threshold: float = 0.95):
        self.similarity_threshold = similarity_threshold
        self.generated_variations = []
        self.image_hashes = set()
        
    def is_duplicate(self, new_image: Image) -> bool:
        """ìƒˆ ì´ë¯¸ì§€ê°€ ê¸°ì¡´ ë³€í˜•ë“¤ê³¼ ì¤‘ë³µì¸ì§€ í™•ì¸"""
        # 1. ì´ë¯¸ì§€ í•´ì‹œ ê¸°ë°˜ ë¹ ë¥¸ ì¤‘ë³µ ê²€ì‚¬
        image_hash = self.calculate_image_hash(new_image)
        if image_hash in self.image_hashes:
            return True
        
        # 2. êµ¬ì¡°ì  ìœ ì‚¬ë„ ê¸°ë°˜ ì •ë°€ ê²€ì‚¬
        for existing_image in self.generated_variations:
            similarity = self.calculate_similarity(new_image, existing_image)
            if similarity > self.similarity_threshold:
                return True
        
        return False
    
    def calculate_image_hash(self, image: Image) -> str:
        """ì´ë¯¸ì§€ì˜ ì§€ê°ì  í•´ì‹œ ê³„ì‚°"""
        import imagehash
        
        # ì—¬ëŸ¬ í•´ì‹œ ì•Œê³ ë¦¬ì¦˜ ì¡°í•© ì‚¬ìš©
        hash_methods = [
            imagehash.average_hash,
            imagehash.phash,
            imagehash.dhash
        ]
        
        hashes = []
        for hash_method in hash_methods:
            hash_value = hash_method(image)
            hashes.append(str(hash_value))
        
        # ì¡°í•©ëœ í•´ì‹œ
        combined_hash = "|".join(hashes)
        return combined_hash
    
    def add_variation(self, image: Image):
        """ìƒˆë¡œìš´ ë³€í˜•ì„ ì¤‘ë³µ ë°©ì§€ ì‹œìŠ¤í…œì— ë“±ë¡"""
        if not self.is_duplicate(image):
            self.generated_variations.append(image.copy())
            self.image_hashes.add(self.calculate_image_hash(image))
            return True
        return False
    
    def clear(self):
        """ë“±ë¡ëœ ë³€í˜•ë“¤ ì´ˆê¸°í™”"""
        self.generated_variations.clear()
        self.image_hashes.clear()
```

## âš¡ ì„±ëŠ¥ ìµœì í™”

### 1. ë©”ëª¨ë¦¬ ê´€ë¦¬
```python
class MemoryOptimizer:
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”"""
    
    def __init__(self, max_memory_mb: int = 2048):
        self.max_memory_mb = max_memory_mb
        self.current_memory_usage = 0
        
    def optimize_image_size(self, image: Image, max_dimension: int = 2048) -> Image:
        """ì´ë¯¸ì§€ í¬ê¸° ìµœì í™”"""
        width, height = image.size
        
        if max(width, height) > max_dimension:
            # ë¹„ë¡€ì  ë¦¬ì‚¬ì´ì¦ˆ
            if width > height:
                new_width = max_dimension
                new_height = int((height * max_dimension) / width)
            else:
                new_height = max_dimension
                new_width = int((width * max_dimension) / height)
            
            optimized = image.resize((new_width, new_height), Image.Resampling.LANCZOS)
            return optimized
        
        return image
    
    def manage_batch_memory(self, batch_size: int, image_sizes: List[tuple]) -> int:
        """ë°°ì¹˜ ì²˜ë¦¬ ì‹œ ë©”ëª¨ë¦¬ ê¸°ë°˜ ìµœì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°"""
        # í‰ê·  ì´ë¯¸ì§€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •
        avg_pixels = sum(w * h for w, h in image_sizes) / len(image_sizes)
        estimated_mb_per_image = (avg_pixels * 3 * 4) / (1024 * 1024)  # RGB, float32
        
        # API ì‘ë‹µ ì´ë¯¸ì§€ê¹Œì§€ ê³ ë ¤í•œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        total_memory_per_batch = estimated_mb_per_image * batch_size * 2  # ì›ë³¸ + ê²°ê³¼
        
        if total_memory_per_batch > self.max_memory_mb:
            optimal_batch_size = max(1, int(self.max_memory_mb / (estimated_mb_per_image * 2)))
            return optimal_batch_size
        
        return batch_size
    
    def cleanup_temp_files(self, temp_dir: Path):
        """ì„ì‹œ íŒŒì¼ ì •ë¦¬"""
        if temp_dir.exists():
            import shutil
            shutil.rmtree(temp_dir, ignore_errors=True)
```

### 2. ìºì‹± ì‹œìŠ¤í…œ
```python
class VariationCache:
    """ë³€í˜• ìƒì„± ê²°ê³¼ ìºì‹±"""
    
    def __init__(self, cache_dir: Path = None, max_cache_size_gb: float = 1.0):
        self.cache_dir = cache_dir or Path.home() / '.nanobanana_cache' / 'variations'
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.max_cache_size_gb = max_cache_size_gb
        self.cache_index = self.load_cache_index()
        
    def generate_cache_key(self, image_path: Path, prompt: str, variation_params: Dict) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        import hashlib
        
        # ì´ë¯¸ì§€ íŒŒì¼ í•´ì‹œ
        with open(image_path, 'rb') as f:
            file_hash = hashlib.md5(f.read()).hexdigest()[:8]
        
        # í”„ë¡¬í”„íŠ¸ ë° íŒŒë¼ë¯¸í„° í•´ì‹œ
        param_str = f"{prompt}_{json.dumps(variation_params, sort_keys=True)}"
        param_hash = hashlib.md5(param_str.encode()).hexdigest()[:8]
        
        return f"{file_hash}_{param_hash}"
    
    def get_cached_result(self, cache_key: str) -> Optional[List[Path]]:
        """ìºì‹œëœ ê²°ê³¼ ì¡°íšŒ"""
        if cache_key in self.cache_index:
            cache_entry = self.cache_index[cache_key]
            result_paths = [Path(path) for path in cache_entry['results']]
            
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if all(path.exists() for path in result_paths):
                # ìµœê·¼ ì‚¬ìš© ì‹œê°„ ì—…ë°ì´íŠ¸
                cache_entry['last_used'] = datetime.now().isoformat()
                self.save_cache_index()
                return result_paths
            else:
                # ëˆ„ë½ëœ íŒŒì¼ì´ ìˆìœ¼ë©´ ìºì‹œ ì—”íŠ¸ë¦¬ ì œê±°
                del self.cache_index[cache_key]
                
        return None
    
    def cache_result(self, cache_key: str, result_paths: List[Path], 
                    original_image: Path, prompt: str, params: Dict):
        """ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥"""
        cache_subdir = self.cache_dir / cache_key
        cache_subdir.mkdir(exist_ok=True)
        
        cached_paths = []
        for i, result_path in enumerate(result_paths):
            if result_path.exists():
                cached_path = cache_subdir / f"variation_{i}{result_path.suffix}"
                shutil.copy2(result_path, cached_path)
                cached_paths.append(str(cached_path))
        
        # ìºì‹œ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
        self.cache_index[cache_key] = {
            'results': cached_paths,
            'original_image': str(original_image),
            'prompt': prompt,
            'params': params,
            'created': datetime.now().isoformat(),
            'last_used': datetime.now().isoformat()
        }
        
        self.save_cache_index()
        self.cleanup_old_cache()
    
    def cleanup_old_cache(self):
        """ì˜¤ë˜ëœ ìºì‹œ ì •ë¦¬"""
        # ìºì‹œ í¬ê¸° í™•ì¸
        total_size = sum(
            sum(f.stat().st_size for f in cache_dir.rglob('*') if f.is_file())
            for cache_dir in self.cache_dir.iterdir()
            if cache_dir.is_dir()
        ) / (1024**3)  # GB ë³€í™˜
        
        if total_size > self.max_cache_size_gb:
            # ì˜¤ë˜ëœ í•­ëª©ë¶€í„° ì œê±°
            sorted_entries = sorted(
                self.cache_index.items(),
                key=lambda x: x[1]['last_used']
            )
            
            for cache_key, entry in sorted_entries:
                if total_size <= self.max_cache_size_gb * 0.8:  # 80%ê¹Œì§€ ì¤„ì´ê¸°
                    break
                    
                # ìºì‹œ ë””ë ‰í† ë¦¬ ì œê±°
                cache_subdir = self.cache_dir / cache_key
                if cache_subdir.exists():
                    shutil.rmtree(cache_subdir)
                
                # ì¸ë±ìŠ¤ì—ì„œ ì œê±°
                del self.cache_index[cache_key]
                
                # ì œê±°ëœ í¬ê¸° ê³„ì‚°
                removed_size = sum(len(path) for path in entry['results']) / (1024**3)
                total_size -= removed_size
            
            self.save_cache_index()
```

## ğŸ”§ ê³ ê¸‰ ì˜¤ë¥˜ ì²˜ë¦¬

### 1. ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜
```python
class RetryManager:
    """ì§€ëŠ¥ì  ì¬ì‹œë„ ì‹œìŠ¤í…œ"""
    
    def __init__(self, max_attempts: int = 3, base_delay: float = 1.0):
        self.max_attempts = max_attempts
        self.base_delay = base_delay
        
    def retry_with_backoff(self, func, *args, **kwargs):
        """ì§€ìˆ˜ ë°±ì˜¤í”„ë¥¼ ì‚¬ìš©í•œ ì¬ì‹œë„"""
        last_exception = None
        
        for attempt in range(self.max_attempts):
            try:
                return func(*args, **kwargs)
            except Exception as e:
                last_exception = e
                
                # ì¬ì‹œë„í•  ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ì²´í¬
                if self.is_non_retryable_error(e):
                    raise e
                
                if attempt < self.max_attempts - 1:
                    delay = self.base_delay * (2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
                    time.sleep(delay)
                    
                    # í”„ë¡¬í”„íŠ¸ ë³€í˜• ì‹œë„
                    if 'prompt' in kwargs:
                        kwargs['prompt'] = self.modify_prompt_for_retry(
                            kwargs['prompt'], attempt + 1
                        )
        
        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨
        raise last_exception
    
    def is_non_retryable_error(self, error: Exception) -> bool:
        """ì¬ì‹œë„í•˜ì§€ ì•Šì„ ì˜¤ë¥˜ íŒë‹¨"""
        non_retryable_patterns = [
            'authentication failed',
            'invalid api key',
            'quota exceeded',
            'content policy violation'
        ]
        
        error_msg = str(error).lower()
        return any(pattern in error_msg for pattern in non_retryable_patterns)
    
    def modify_prompt_for_retry(self, original_prompt: str, attempt: int) -> str:
        """ì¬ì‹œë„ ì‹œ í”„ë¡¬í”„íŠ¸ ë³€í˜•"""
        variations = [
            f"{original_prompt} (alternative interpretation)",
            f"Create a different version of: {original_prompt}",
            f"Generate an alternative approach to: {original_prompt}",
            f"Please try again with: {original_prompt}"
        ]
        
        if attempt - 1 < len(variations):
            return variations[attempt - 1]
        return original_prompt
```

### 2. ì ì§„ì  í’ˆì§ˆ ì¡°ì •
```python
class AdaptiveQualityManager:
    """ì ì§„ì  í’ˆì§ˆ ì¡°ì • ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.success_rate_history = []
        self.current_quality_level = 'high'
        self.quality_settings = {
            'high': {
                'max_image_size': 2048,
                'quality_threshold': 0.8,
                'max_attempts_per_variation': 3
            },
            'medium': {
                'max_image_size': 1536,
                'quality_threshold': 0.6,
                'max_attempts_per_variation': 2
            },
            'low': {
                'max_image_size': 1024,
                'quality_threshold': 0.4,
                'max_attempts_per_variation': 1
            }
        }
    
    def adjust_quality_based_on_performance(self, success_rate: float):
        """ì„±ëŠ¥ ê¸°ë°˜ í’ˆì§ˆ ìˆ˜ì¤€ ì¡°ì •"""
        self.success_rate_history.append(success_rate)
        
        # ìµœê·¼ 5íšŒ í‰ê·  ê³„ì‚°
        if len(self.success_rate_history) >= 5:
            recent_avg = sum(self.success_rate_history[-5:]) / 5
            
            # í’ˆì§ˆ ìˆ˜ì¤€ ì¡°ì •
            if recent_avg < 0.5 and self.current_quality_level == 'high':
                self.current_quality_level = 'medium'
                logging.info("í’ˆì§ˆ ìˆ˜ì¤€ì„ ì¤‘ê°„ìœ¼ë¡œ ë‚®ì¶¤ (ì„±ëŠ¥ ê°œì„ ì„ ìœ„í•´)")
            elif recent_avg < 0.3 and self.current_quality_level == 'medium':
                self.current_quality_level = 'low'
                logging.info("í’ˆì§ˆ ìˆ˜ì¤€ì„ ë‚®ìŒìœ¼ë¡œ ì¡°ì • (ì•ˆì •ì„± í™•ë³´ë¥¼ ìœ„í•´)")
            elif recent_avg > 0.8 and self.current_quality_level != 'high':
                self.current_quality_level = 'high'
                logging.info("ì„±ëŠ¥ì´ ì¢‹ì•„ì ¸ í’ˆì§ˆ ìˆ˜ì¤€ì„ ë†’ìŒìœ¼ë¡œ ë³µì›")
    
    def get_current_settings(self) -> Dict:
        """í˜„ì¬ í’ˆì§ˆ ì„¤ì • ë°˜í™˜"""
        return self.quality_settings[self.current_quality_level].copy()
```

## ğŸ”— ImageVariationProcessor í†µí•©
```python
class EnhancedImageVariationProcessor(ImageVariationProcessor):
    """ê³ ê¸‰ ê¸°ëŠ¥ì´ í†µí•©ëœ ì´ë¯¸ì§€ ë³€í˜• í”„ë¡œì„¸ì„œ"""
    
    def __init__(self, api_key: str, model: str = "gemini-2.5-flash-image-preview"):
        super().__init__(api_key, model)
        
        # ê³ ê¸‰ ê¸°ëŠ¥ ëª¨ë“ˆë“¤
        self.quality_analyzer = VariationQualityAnalyzer()
        self.duplication_preventer = DuplicationPreventer()
        self.memory_optimizer = MemoryOptimizer()
        self.retry_manager = RetryManager()
        self.cache = VariationCache()
        self.quality_manager = AdaptiveQualityManager()
    
    def generate_variations_enhanced(self, image_path: Path, count: int,
                                   **kwargs) -> Dict[str, any]:
        """ê³ ê¸‰ ê¸°ëŠ¥ì´ ì ìš©ëœ ë³€í˜• ìƒì„±"""
        # ìºì‹œ í™•ì¸
        cache_key = self.cache.generate_cache_key(
            image_path, kwargs.get('prompt', ''), kwargs
        )
        
        cached_result = self.cache.get_cached_result(cache_key)
        if cached_result:
            logging.info(f"ìºì‹œëœ ê²°ê³¼ ì‚¬ìš©: {cache_key}")
            return self.format_cached_result(cached_result)
        
        # ë©”ëª¨ë¦¬ ìµœì í™”
        original_image = Image.open(image_path)
        optimized_image = self.memory_optimizer.optimize_image_size(original_image)
        
        # í’ˆì§ˆ ì„¤ì • ì ìš©
        quality_settings = self.quality_manager.get_current_settings()
        
        results = {
            'successful': 0,
            'failed': 0,
            'variations': [],
            'quality_scores': []
        }
        
        # ì¤‘ë³µ ë°©ì§€ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.duplication_preventer.clear()
        
        # ë³€í˜• ìƒì„± (í’ˆì§ˆ ê¸°ë°˜ ì¬ì‹œë„ í¬í•¨)
        attempts = 0
        while results['successful'] < count and attempts < count * quality_settings['max_attempts_per_variation']:
            try:
                # ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ ë³€í˜• ìƒì„±
                variation_result = self.retry_manager.retry_with_backoff(
                    self._generate_single_variation_with_quality_check,
                    optimized_image,
                    attempts,
                    quality_settings,
                    **kwargs
                )
                
                if variation_result['success']:
                    results['successful'] += 1
                    results['variations'].append(variation_result)
                    results['quality_scores'].append(variation_result['quality_metrics'])
                else:
                    results['failed'] += 1
                
            except Exception as e:
                results['failed'] += 1
                logging.error(f"ë³€í˜• ìƒì„± ì‹¤íŒ¨ (ì‹œë„ {attempts + 1}): {e}")
            
            attempts += 1
        
        # ì„±ëŠ¥ ê¸°ë°˜ í’ˆì§ˆ ì¡°ì •
        success_rate = results['successful'] / max(1, results['successful'] + results['failed'])
        self.quality_manager.adjust_quality_based_on_performance(success_rate)
        
        # ê²°ê³¼ ìºì‹±
        if results['variations']:
            result_paths = [Path(v['output_file']) for v in results['variations']]
            self.cache.cache_result(cache_key, result_paths, image_path, 
                                  kwargs.get('prompt', ''), kwargs)
        
        return results
    
    def _generate_single_variation_with_quality_check(self, image: Image, 
                                                     variation_id: int,
                                                     quality_settings: Dict,
                                                     **kwargs) -> Dict:
        """í’ˆì§ˆ ê²€ì¦ì´ í¬í•¨ëœ ë‹¨ì¼ ë³€í˜• ìƒì„±"""
        # ê¸°ë³¸ ë³€í˜• ìƒì„±
        variation_result = self._generate_basic_variation(
            image, variation_id, **kwargs
        )
        
        if not variation_result['success']:
            return variation_result
        
        # ìƒì„±ëœ ì´ë¯¸ì§€ ë¡œë“œ
        generated_image = Image.open(variation_result['output_file'])
        
        # ì¤‘ë³µ ê²€ì‚¬
        if self.duplication_preventer.is_duplicate(generated_image):
            return {
                'success': False,
                'error': 'ì¤‘ë³µëœ ë³€í˜•ì´ ìƒì„±ë¨',
                'quality_metrics': {}
            }
        
        # í’ˆì§ˆ ë¶„ì„
        other_variations = [img for img in self.duplication_preventer.generated_variations]
        quality_metrics = self.quality_analyzer.analyze_variation_quality(
            image, generated_image, other_variations
        )
        
        # í’ˆì§ˆ ê¸°ì¤€ í™•ì¸
        if (quality_metrics['overall_quality'] >= quality_settings['quality_threshold'] 
            and self.quality_analyzer.is_acceptable_quality(quality_metrics)):
            
            # ì¤‘ë³µ ë°©ì§€ ì‹œìŠ¤í…œì— ë“±ë¡
            self.duplication_preventer.add_variation(generated_image)
            
            variation_result['quality_metrics'] = quality_metrics
            return variation_result
        else:
            # í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬
            Path(variation_result['output_file']).unlink(missing_ok=True)
            return {
                'success': False,
                'error': f"í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬ (ì ìˆ˜: {quality_metrics['overall_quality']:.2f})",
                'quality_metrics': quality_metrics
            }
```

## âœ… ì™„ë£Œ ê¸°ì¤€
- [ ] ë³€í˜• í’ˆì§ˆ ë¶„ì„ ì‹œìŠ¤í…œ êµ¬í˜„ ë° í…ŒìŠ¤íŠ¸
- [ ] ì¤‘ë³µ ë°©ì§€ ì•Œê³ ë¦¬ì¦˜ ê°œë°œ ë° ê²€ì¦
- [ ] ë©”ëª¨ë¦¬ ìµœì í™” ë° ì„±ëŠ¥ íŠœë‹
- [ ] ìºì‹± ì‹œìŠ¤í…œ êµ¬í˜„ ë° í…ŒìŠ¤íŠ¸
- [ ] ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ ë° ì˜¤ë¥˜ ë³µêµ¬ ì‹œìŠ¤í…œ
- [ ] ì ì§„ì  í’ˆì§ˆ ì¡°ì • ì‹œìŠ¤í…œ êµ¬í˜„
- [ ] í†µí•©ëœ EnhancedImageVariationProcessor í…ŒìŠ¤íŠ¸

## ğŸ”„ ë‹¤ìŒ ë‹¨ê³„
Phase 5ì—ì„œëŠ” ì „ì²´ ì‹œìŠ¤í…œì˜ í…ŒìŠ¤íŒ…, ë¬¸ì„œí™”, ê·¸ë¦¬ê³  ë°°í¬ ì¤€ë¹„ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.