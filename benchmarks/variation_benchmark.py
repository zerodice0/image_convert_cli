#!/usr/bin/env python3
"""
Performance benchmark for image variation functionality
"""

import time
import statistics
import tempfile
from pathlib import Path
from PIL import Image
import psutil
import os
import json
import sys
from typing import List, Dict, Any

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from batch_nanobanana_core import (
    VariationEngine, 
    VariationPromptGenerator,
    ImageVariationProcessor
)


class VariationBenchmark:
    """Performance benchmark for image variation system"""
    
    def __init__(self):
        self.results = {}
        self.process = psutil.Process(os.getpid())
        self.temp_dir = Path(tempfile.mkdtemp())
    
    def benchmark_prompt_generation(self, iterations: int = 1000):
        """Benchmark prompt generation performance"""
        print(f"ğŸ”„ Benchmarking prompt generation ({iterations} iterations)...")
        
        generator = VariationPromptGenerator()
        engine = VariationEngine()
        test_image = Image.new('RGB', (512, 512), 'blue')
        
        variation_types = list(engine.VARIATION_TYPES.keys())
        
        # Benchmark different variation types
        type_results = {}
        
        for variation_type in variation_types:
            times = []
            
            for i in range(iterations // len(variation_types)):
                start_time = time.perf_counter()
                
                prompt = engine.create_variation_prompt(
                    base_image=test_image,
                    variation_id=i,
                    variation_type=variation_type,
                    seed=42
                )
                
                end_time = time.perf_counter()
                times.append(end_time - start_time)
            
            type_results[variation_type] = {
                'avg_time': statistics.mean(times) * 1000,  # ms
                'min_time': min(times) * 1000,
                'max_time': max(times) * 1000,
                'std_dev': statistics.stdev(times) * 1000,
                'iterations': len(times)
            }
        
        self.results['prompt_generation'] = type_results
        print(f"âœ… Prompt generation benchmark completed")
    
    def benchmark_image_processing(self, image_sizes: List[tuple]):
        """Benchmark image processing performance"""
        print(f"ğŸ–¼ï¸ Benchmarking image processing...")
        
        processing_results = {}
        
        for size in image_sizes:
            print(f"  Testing {size[0]}x{size[1]} images...")
            
            # Create test image
            test_image = Image.new('RGB', size, (100, 150, 200))
            image_path = self.temp_dir / f"test_{size[0]}x{size[1]}.png"
            test_image.save(image_path)
            
            try:
                # Memory usage before
                memory_before = self.process.memory_info().rss / 1024 / 1024
                
                # Time image loading and basic processing
                start_time = time.perf_counter()
                
                # Simulate image processing operations
                loaded_image = Image.open(image_path)
                resized = loaded_image.resize((size[0] // 2, size[1] // 2))
                converted = loaded_image.convert('RGB')
                
                end_time = time.perf_counter()
                memory_after = self.process.memory_info().rss / 1024 / 1024
                
                processing_results[f"{size[0]}x{size[1]}"] = {
                    'processing_time': (end_time - start_time) * 1000,  # ms
                    'memory_usage': memory_after - memory_before,  # MB
                    'image_size_mb': image_path.stat().st_size / 1024 / 1024
                }
                
            finally:
                # Cleanup
                if image_path.exists():
                    image_path.unlink()
        
        self.results['image_processing'] = processing_results
        print(f"âœ… Image processing benchmark completed")
    
    def benchmark_quality_analysis(self, iterations: int = 100):
        """Benchmark quality analysis performance"""
        print(f"ğŸ” Benchmarking quality analysis...")
        
        try:
            from variation_advanced import VariationQualityAnalyzer
            analyzer = VariationQualityAnalyzer()
            
            # Create test images
            img1 = Image.new('RGB', (512, 512), 'red')
            img2 = Image.new('RGB', (512, 512), 'blue')
            img3 = Image.new('RGB', (512, 512), 'green')
            variations = [img2, img3]
            
            times = []
            for i in range(iterations):
                start_time = time.perf_counter()
                
                metrics = analyzer.analyze_variation_quality(img1, img2, variations)
                
                end_time = time.perf_counter()
                times.append(end_time - start_time)
            
            self.results['quality_analysis'] = {
                'avg_time': statistics.mean(times) * 1000,  # ms
                'min_time': min(times) * 1000,
                'max_time': max(times) * 1000,
                'std_dev': statistics.stdev(times) * 1000,
                'throughput': len(times) / sum(times),  # analyses per second
                'iterations': iterations
            }
            
            print(f"âœ… Quality analysis benchmark completed")
            
        except ImportError:
            print("âš ï¸ Advanced quality analysis not available")
            self.results['quality_analysis'] = {
                'status': 'unavailable',
                'reason': 'variation_advanced module not found'
            }
    
    def benchmark_memory_usage(self, batch_sizes: List[int]):
        """Benchmark memory usage with different batch sizes"""
        print(f"ğŸ’¾ Benchmarking memory usage...")
        
        memory_results = {}
        
        for batch_size in batch_sizes:
            print(f"  Testing batch size: {batch_size}")
            
            # Initial memory
            initial_memory = self.process.memory_info().rss / 1024 / 1024
            
            # Create batch of images
            images = []
            for i in range(batch_size):
                img = Image.new('RGB', (1024, 1024), (i * 25 % 255, i * 35 % 255, i * 45 % 255))
                images.append(img)
            
            # Peak memory
            peak_memory = self.process.memory_info().rss / 1024 / 1024
            
            # Process images (simulate variation operations)
            processed = []
            for img in images:
                # Simulate processing
                resized = img.resize((512, 512))
                converted = resized.convert('RGB')
                processed.append(converted)
            
            # Final memory
            final_memory = self.process.memory_info().rss / 1024 / 1024
            
            memory_results[f"batch_{batch_size}"] = {
                'initial_memory': initial_memory,
                'peak_memory': peak_memory,
                'final_memory': final_memory,
                'memory_increase': peak_memory - initial_memory,
                'memory_per_image': (peak_memory - initial_memory) / batch_size if batch_size > 0 else 0
            }
            
            # Cleanup
            del images
            del processed
        
        self.results['memory_usage'] = memory_results
        print(f"âœ… Memory usage benchmark completed")
    
    def benchmark_concurrent_processing(self, worker_counts: List[int]):
        """Benchmark concurrent processing simulation"""
        print(f"ğŸ”€ Benchmarking concurrent processing...")
        
        import threading
        import queue
        
        def worker(task_queue: queue.Queue, result_queue: queue.Queue):
            """Worker function for concurrent processing"""
            engine = VariationEngine()
            test_image = Image.new('RGB', (256, 256), 'white')
            
            while True:
                try:
                    task_id = task_queue.get(timeout=1)
                    
                    # Simulate processing
                    start_time = time.perf_counter()
                    prompt = engine.create_variation_prompt(
                        base_image=test_image,
                        variation_id=task_id,
                        variation_type='random'
                    )
                    end_time = time.perf_counter()
                    
                    result_queue.put({
                        'task_id': task_id,
                        'processing_time': end_time - start_time,
                        'success': True
                    })
                    
                    task_queue.task_done()
                    
                except queue.Empty:
                    break
                except Exception as e:
                    result_queue.put({
                        'task_id': task_id,
                        'error': str(e),
                        'success': False
                    })
        
        concurrent_results = {}
        total_tasks = 100
        
        for worker_count in worker_counts:
            print(f"  Testing {worker_count} workers...")
            
            task_queue = queue.Queue()
            result_queue = queue.Queue()
            
            # Add tasks
            for i in range(total_tasks):
                task_queue.put(i)
            
            # Start workers
            threads = []
            start_time = time.perf_counter()
            
            for _ in range(worker_count):
                thread = threading.Thread(target=worker, args=(task_queue, result_queue))
                thread.start()
                threads.append(thread)
            
            # Wait for completion
            task_queue.join()
            
            # Stop workers
            for thread in threads:
                thread.join(timeout=1)
            
            end_time = time.perf_counter()
            
            # Collect results
            results = []
            while not result_queue.empty():
                results.append(result_queue.get())
            
            successful = sum(1 for r in results if r.get('success', False))
            total_time = end_time - start_time
            
            concurrent_results[f"{worker_count}_workers"] = {
                'total_time': total_time,
                'successful_tasks': successful,
                'failed_tasks': len(results) - successful,
                'throughput': successful / total_time,
                'avg_time_per_task': total_time / successful if successful > 0 else 0
            }
        
        self.results['concurrent_processing'] = concurrent_results
        print(f"âœ… Concurrent processing benchmark completed")
    
    def generate_report(self) -> str:
        """Generate comprehensive benchmark report"""
        report = "# ì´ë¯¸ì§€ ë³€í˜• ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë¦¬í¬íŠ¸\n\n"
        report += f"**ì‹¤í–‰ ì‹œê°„**: {time.strftime('%Y-%m-%d %H:%M:%S')}\n"
        report += f"**ì‹œìŠ¤í…œ ì •ë³´**: {psutil.cpu_count()} CPU cores, {psutil.virtual_memory().total // 1024**3:.1f}GB RAM\n\n"
        
        # Prompt generation results
        if 'prompt_generation' in self.results:
            report += "## ğŸ“ í”„ë¡¬í”„íŠ¸ ìƒì„± ì„±ëŠ¥\n\n"
            for vtype, metrics in self.results['prompt_generation'].items():
                report += f"### {vtype}\n"
                report += f"- í‰ê·  ì‹œê°„: {metrics['avg_time']:.2f}ms\n"
                report += f"- ìµœì†Œ ì‹œê°„: {metrics['min_time']:.2f}ms\n"
                report += f"- ìµœëŒ€ ì‹œê°„: {metrics['max_time']:.2f}ms\n"
                report += f"- í‘œì¤€í¸ì°¨: {metrics['std_dev']:.2f}ms\n"
                report += f"- ë°˜ë³µ íšŸìˆ˜: {metrics['iterations']}\n\n"
        
        # Image processing results
        if 'image_processing' in self.results:
            report += "## ğŸ–¼ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ì„±ëŠ¥\n\n"
            for size, metrics in self.results['image_processing'].items():
                report += f"### {size}\n"
                report += f"- ì²˜ë¦¬ ì‹œê°„: {metrics['processing_time']:.2f}ms\n"
                report += f"- ë©”ëª¨ë¦¬ ì‚¬ìš©: {metrics['memory_usage']:.1f}MB\n"
                report += f"- ì´ë¯¸ì§€ í¬ê¸°: {metrics['image_size_mb']:.1f}MB\n\n"
        
        # Quality analysis results
        if 'quality_analysis' in self.results:
            qa = self.results['quality_analysis']
            if qa.get('status') == 'unavailable':
                report += "## ğŸ” í’ˆì§ˆ ë¶„ì„ ì„±ëŠ¥\n\n"
                report += f"âš ï¸ {qa.get('reason', 'ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ')}\n\n"
            else:
                report += "## ğŸ” í’ˆì§ˆ ë¶„ì„ ì„±ëŠ¥\n\n"
                report += f"- í‰ê·  ì‹œê°„: {qa['avg_time']:.2f}ms\n"
                report += f"- ìµœì†Œ ì‹œê°„: {qa['min_time']:.2f}ms\n"
                report += f"- ìµœëŒ€ ì‹œê°„: {qa['max_time']:.2f}ms\n"
                report += f"- ì²˜ë¦¬ëŸ‰: {qa['throughput']:.1f} ë¶„ì„/ì´ˆ\n"
                report += f"- ë°˜ë³µ íšŸìˆ˜: {qa['iterations']}\n\n"
        
        # Memory usage results
        if 'memory_usage' in self.results:
            report += "## ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰\n\n"
            for batch, metrics in self.results['memory_usage'].items():
                report += f"### {batch.replace('_', ' ').title()}\n"
                report += f"- ì´ˆê¸° ë©”ëª¨ë¦¬: {metrics['initial_memory']:.1f}MB\n"
                report += f"- ìµœëŒ€ ë©”ëª¨ë¦¬: {metrics['peak_memory']:.1f}MB\n"
                report += f"- ë©”ëª¨ë¦¬ ì¦ê°€: {metrics['memory_increase']:.1f}MB\n"
                report += f"- ì´ë¯¸ì§€ë‹¹ ë©”ëª¨ë¦¬: {metrics['memory_per_image']:.2f}MB\n\n"
        
        # Concurrent processing results
        if 'concurrent_processing' in self.results:
            report += "## ğŸ”€ ë™ì‹œ ì²˜ë¦¬ ì„±ëŠ¥\n\n"
            for config, metrics in self.results['concurrent_processing'].items():
                report += f"### {config.replace('_', ' ').title()}\n"
                report += f"- ì´ ì‹œê°„: {metrics['total_time']:.2f}ì´ˆ\n"
                report += f"- ì„±ê³µí•œ ì‘ì—…: {metrics['successful_tasks']}\n"
                report += f"- ì‹¤íŒ¨í•œ ì‘ì—…: {metrics['failed_tasks']}\n"
                report += f"- ì²˜ë¦¬ëŸ‰: {metrics['throughput']:.1f} ì‘ì—…/ì´ˆ\n"
                report += f"- ì‘ì—…ë‹¹ í‰ê·  ì‹œê°„: {metrics['avg_time_per_task']:.2f}ì´ˆ\n\n"
        
        # Performance recommendations
        report += "## ğŸ“Š ì„±ëŠ¥ ê¶Œì¥ì‚¬í•­\n\n"
        report += self._generate_recommendations()
        
        return report
    
    def _generate_recommendations(self) -> str:
        """Generate performance recommendations based on results"""
        recommendations = []
        
        # Analyze prompt generation
        if 'prompt_generation' in self.results:
            avg_times = [metrics['avg_time'] for metrics in self.results['prompt_generation'].values()]
            max_avg_time = max(avg_times)
            
            if max_avg_time > 50:  # ms
                recommendations.append("- í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹œê°„ì´ ê¸¸ì–´ ìºì‹± ì‹œìŠ¤í…œ ë„ì…ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
            else:
                recommendations.append("- í”„ë¡¬í”„íŠ¸ ìƒì„± ì„±ëŠ¥ì´ ì–‘í˜¸í•©ë‹ˆë‹¤.")
        
        # Analyze memory usage
        if 'memory_usage' in self.results:
            max_increase = max(
                metrics['memory_increase'] 
                for metrics in self.results['memory_usage'].values()
            )
            
            if max_increase > 1000:  # MB
                recommendations.append("- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ì•„ ì´ë¯¸ì§€ í¬ê¸° ìµœì í™”ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")
            elif max_increase > 500:
                recommendations.append("- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë³´í†µ ìˆ˜ì¤€ì…ë‹ˆë‹¤. ë°°ì¹˜ í¬ê¸° ì¡°ì •ì„ ê³ ë ¤í•˜ì„¸ìš”.")
            else:
                recommendations.append("- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ íš¨ìœ¨ì ì…ë‹ˆë‹¤.")
        
        # Analyze concurrent processing
        if 'concurrent_processing' in self.results:
            throughputs = [
                metrics['throughput'] 
                for metrics in self.results['concurrent_processing'].values()
            ]
            
            if len(throughputs) > 1:
                best_throughput = max(throughputs)
                best_config = max(
                    self.results['concurrent_processing'].items(),
                    key=lambda x: x[1]['throughput']
                )[0]
                
                recommendations.append(f"- ìµœì  ë™ì‹œ ì²˜ë¦¬ ì„¤ì •: {best_config} (ì²˜ë¦¬ëŸ‰: {best_throughput:.1f} ì‘ì—…/ì´ˆ)")
        
        if not recommendations:
            recommendations.append("- ì „ì²´ì ìœ¼ë¡œ ì•ˆì •ì ì¸ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.")
        
        return "\n".join(recommendations) + "\n"
    
    def save_results(self, filepath: Path):
        """Save benchmark results to JSON file"""
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
    
    def cleanup(self):
        """Clean up temporary files"""
        import shutil
        if self.temp_dir.exists():
            shutil.rmtree(self.temp_dir)


def main():
    """Run comprehensive benchmark suite"""
    print("ğŸš€ ì´ë¯¸ì§€ ë³€í˜• ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘\n")
    
    benchmark = VariationBenchmark()
    
    try:
        # Run benchmarks
        benchmark.benchmark_prompt_generation(iterations=500)
        benchmark.benchmark_image_processing([(512, 512), (1024, 1024), (2048, 2048)])
        benchmark.benchmark_quality_analysis(iterations=50)
        benchmark.benchmark_memory_usage([1, 5, 10, 20])
        benchmark.benchmark_concurrent_processing([1, 2, 4, 8])
        
        # Generate and save report
        report = benchmark.generate_report()
        print("\n" + "="*60)
        print("ğŸ“ˆ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼")
        print("="*60)
        print(report)
        
        # Save to files
        results_dir = Path(__file__).parent.parent / "benchmark_results"
        results_dir.mkdir(exist_ok=True)
        
        timestamp = time.strftime('%Y%m%d_%H%M%S')
        report_file = results_dir / f"benchmark_report_{timestamp}.md"
        results_file = results_dir / f"benchmark_results_{timestamp}.json"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        benchmark.save_results(results_file)
        
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ë¨:")
        print(f"  ë¦¬í¬íŠ¸: {report_file}")
        print(f"  ë°ì´í„°: {results_file}")
        
    finally:
        benchmark.cleanup()


if __name__ == '__main__':
    main()